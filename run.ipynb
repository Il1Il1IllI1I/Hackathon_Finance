{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# 티커와 그에 해당하는 이름을 정의합니다.\n",
    "# 사용자는 이 딕셔너리에 새로운 티커와 이름을 추가할 수 있습니다.\n",
    "tickers = {\n",
    "    \"CL=F\": \"WTI\",\n",
    "    \"USDKRW=X\": \"USDKRW\",\n",
    "    \"^KS11\": \"KOSPI200\",\n",
    "    \"^VIX\": \"VIX\"\n",
    "}\n",
    "\n",
    "data = pd.DataFrame()  # 빈 DataFrame을 초기화합니다.\n",
    "\n",
    "# 각 티커에 대한 데이터를 다운로드하고 단일 DataFrame으로 병합합니다.\n",
    "for ticker, name in tickers.items():\n",
    "    series = yf.download(ticker, start=\"2006-12-30\", end=\"2023-09-21\")['Close']  # 각 티커에 대한 종가 데이터를 다운로드합니다.\n",
    "    series.name = name  # 다운로드한 시리즈의 이름을 설정합니다.\n",
    "    if data.empty:  # 첫 번째 티커의 경우, data DataFrame을 초기화합니다.\n",
    "        data = pd.DataFrame(series).reset_index()\n",
    "    else:  # 그 이후의 티커에 대해서는 data DataFrame과 병합합니다.\n",
    "        data = pd.merge(data, pd.DataFrame(series).reset_index(), on='Date', how='outer')\n",
    "\n",
    "# 'Date' 컬럼의 이름을 변경하고 인덱스로 설정합니다.\n",
    "data.rename(columns={'Date': 'Date'}, inplace=True)\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# 누락된 데이터를 처리합니다.\n",
    "data.fillna(method='ffill', inplace=True)  # 누락된 데이터를 앞쪽으로 채웁니다.\n",
    "data.fillna(method='bfill', inplace=True)  # 시작 부분의 누락된 데이터를 뒤쪽으로 채웁니다.\n",
    "\n",
    "# KOSPI 200에 대한 포워드 스테이지를 계산합니다.\n",
    "forward_days = 60\n",
    "data['Forward_Return'] = data['KOSPI200'].shift(-forward_days) / data['KOSPI200'] - 1  # 포워드 리턴을 계산합니다.\n",
    "data['forward_stage'] = pd.cut(data['Forward_Return'], bins=[-float('inf'), 0, 0.04, float('inf')], labels=['down', 'neutral', 'up'])  # 포워드 리턴을 기반으로 스테이지를 분류합니다.\n",
    "\n",
    "# 'forward_stage'에서 NaN이 있는 행을 삭제합니다.\n",
    "data.dropna(subset=['forward_stage'], inplace=True)\n",
    "\n",
    "# 숫자형 컬럼을 2소수점 자리로 반올림합니다.\n",
    "numerical_columns = [name for name in tickers.values() if name != 'KOSPI200']  # 'KOSPI200'을 제외한 모든 컬럼을 선택합니다.\n",
    "data[numerical_columns] = data[numerical_columns].round(2)  # 선택한 컬럼을 반올림합니다.\n",
    "\n",
    "# 인덱스를 재설정하고 'Date' 컬럼을 형식화합니다.\n",
    "data.reset_index(inplace=True)\n",
    "data['Date'] = data['Date'].dt.strftime('%y-%m-%d')\n",
    "\n",
    "# 'Forward_Return' 컬럼을 삭제하고 컬럼 순서를 재배열합니다.\n",
    "data = data[['Date', 'forward_stage'] + numerical_columns]\n",
    "\n",
    "# 필요한 경우 CSV로 저장합니다.\n",
    "data.to_csv('daily_data_value.csv', index=False)\n",
    "\n",
    "# DataFrame의 처음 몇 행을 출력합니다.\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "       Date forward_stage    WTI  USDKRW    VIX\n",
      "0  07-01-02            상승  61.05  914.42  12.04\n",
      "1  07-01-03            상승  58.32  914.31  12.04\n",
      "2  07-01-04            상승  55.59  925.24  11.51\n",
      "3  07-01-05            상승  56.31  925.44  12.14\n",
      "4  07-01-08            상승  56.09  924.37  12.00\n"
     ]
    }
   ],
   "source": [
    "# 상승, 하락으로 label\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# 티커와 그에 해당하는 이름을 정의합니다.\n",
    "tickers = {\n",
    "    \"CL=F\": \"WTI\",\n",
    "    \"USDKRW=X\": \"USDKRW\",\n",
    "    \"^KS11\": \"KOSPI200\",\n",
    "    \"^VIX\": \"VIX\"\n",
    "}\n",
    "\n",
    "data = pd.DataFrame() \n",
    "\n",
    "# 각 티커에 대한 데이터를 다운로드하고 단일 DataFrame으로 병합합니다.\n",
    "for ticker, name in tickers.items():\n",
    "    series = yf.download(ticker, start=\"2006-12-30\", end=\"2023-09-21\")['Close']\n",
    "    series.name = name  \n",
    "    if data.empty:  \n",
    "        data = pd.DataFrame(series).reset_index()\n",
    "    else:  \n",
    "        data = pd.merge(data, pd.DataFrame(series).reset_index(), on='Date', how='outer')\n",
    "\n",
    "# 'Date' 컬럼의 이름을 변경하고 인덱스로 설정합니다.\n",
    "data.rename(columns={'Date': 'Date'}, inplace=True)\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# 누락된 데이터를 처리합니다.\n",
    "data.fillna(method='ffill', inplace=True)  \n",
    "data.fillna(method='bfill', inplace=True)  \n",
    "\n",
    "# KOSPI 200에 대한 포워드 스테이지를 계산합니다.\n",
    "forward_days = 60\n",
    "data['Forward_Return'] = data['KOSPI200'].shift(-forward_days) / data['KOSPI200'] - 1  \n",
    "\n",
    "# 포워드 리턴을 기반으로 스테이지를 분류합니다.\n",
    "data['forward_stage'] = pd.cut(data['Forward_Return'], bins=[-float('inf'), 0, float('inf')], labels=['하락', '상승'])\n",
    "\n",
    "# 'forward_stage'에서 NaN이 있는 행을 삭제합니다.\n",
    "data.dropna(subset=['forward_stage'], inplace=True)\n",
    "\n",
    "# 숫자형 컬럼을 2소수점 자리로 반올림합니다.\n",
    "numerical_columns = [name for name in tickers.values() if name != 'KOSPI200']  \n",
    "data[numerical_columns] = data[numerical_columns].round(2)\n",
    "\n",
    "# 인덱스를 재설정하고 'Date' 컬럼을 형식화합니다.\n",
    "data.reset_index(inplace=True)\n",
    "data['Date'] = data['Date'].dt.strftime('%y-%m-%d')\n",
    "\n",
    "# 'Forward_Return' 컬럼을 삭제하고 컬럼 순서를 재배열합니다.\n",
    "data = data[['Date', 'forward_stage'] + numerical_columns]\n",
    "\n",
    "# 필요한 경우 CSV로 저장합니다.\n",
    "data.to_csv('updown.csv', index=False)\n",
    "\n",
    "# DataFrame의 처음 몇 행을 출력합니다.\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정상성 띄게끔 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\성현태\\AppData\\Local\\Temp\\ipykernel_21780\\843910113.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  model_data['Date'] = pd.to_datetime(model_data['Date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_leaf_nodes': 50, 'max_features': 3, 'max_depth': 15}\n",
      "\n",
      "<10-fold cross-validation>\n",
      "accuracy score mean:  0.8025559105431309\n",
      "\n",
      "<AI model: machine learning done >\n",
      "accuracy_score of train data(0.8 of sample):  0.8632006970665118\n",
      "accuracy_score of test data(0.2 of sample):  0.8488372093023255\n",
      "\n",
      "<Confusion matrix>\n",
      "(of test)\n",
      "up neutral down\n",
      "[[465   0  47]\n",
      " [  0   0   0]\n",
      " [ 83   0 265]]\n",
      "(of all)\n",
      "up neutral down\n",
      "[[2300    0  200]\n",
      " [   0    0    0]\n",
      " [ 401    0 1402]]\n",
      "\n",
      "<Feature importance>\n",
      "WTI :  0.4610167006909029\n",
      "USDKRW :  0.2892442815364459\n",
      "VIX :  0.24973901777265106\n",
      "\n",
      "< AI model: save >\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, cross_val_score  # 여기에 cross_val_score를 추가합니다.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "# 올바른 파일 경로로 데이터를 불러옵니다.\n",
    "model_data = pd.read_csv('updated_updown.csv', encoding='cp949')\n",
    "\n",
    "# 'Date' 칼럼을 인덱스로 설정하고 datetime 타입으로 변환합니다.\n",
    "model_data['Date'] = pd.to_datetime(model_data['Date'])\n",
    "model_data.set_index('Date', inplace=True)\n",
    "\n",
    "# 데이터를 날짜 기준으로 정렬합니다.\n",
    "model_data.sort_index(inplace=True)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 나눌 기준 날짜를 설정합니다.\n",
    "split_date = model_data.index[int(0.8 * len(model_data))]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 나눕니다.\n",
    "train_df = model_data[model_data.index <= split_date]\n",
    "test_df = model_data[model_data.index > split_date]\n",
    "\n",
    "# 특성과 레이블을 준비합니다.\n",
    "X_train = train_df.drop(columns=['forward_stage'])\n",
    "y_train = train_df['forward_stage']\n",
    "X_test = test_df.drop(columns=['forward_stage'])\n",
    "y_test = test_df['forward_stage']\n",
    "\n",
    "# 모델을 초기화합니다.\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=42)\n",
    "\n",
    "# 하이퍼파라미터 그리드를 정의합니다.\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 500],\n",
    "    'max_leaf_nodes': [20, 30, 40, 50],\n",
    "    'max_features': [1, 2, 3],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위해 RandomizedSearchCV를 사용합니다.\n",
    "# 시계열 데이터의 특성을 고려하여 TimeSeriesSplit을 사용합니다.\n",
    "time_split_cv = TimeSeriesSplit(n_splits=10)\n",
    "rnd_search = RandomizedSearchCV(rnd_clf, param_dist_rf, cv=time_split_cv, random_state=42)\n",
    "rnd_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델을 훈련합니다.\n",
    "best_clf = rnd_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# 모델을 평가합니다.\n",
    "train_score = best_clf.score(X_train, y_train)\n",
    "test_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "# 10-fold cross-validation의 정확도를 계산합니다.\n",
    "cv_scores = cross_val_score(best_clf, X_train, y_train, cv=time_split_cv, scoring='accuracy')\n",
    "cv_mean_score = cv_scores.mean()\n",
    "\n",
    "# 혼동 행렬을 확인합니다.\n",
    "y_test_pred = best_clf.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "y_all_pred = best_clf.predict(model_data.drop(columns=['forward_stage']))\n",
    "cm_all = confusion_matrix(model_data['forward_stage'], y_all_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "\n",
    "# 특성 중요도를 확인합니다.\n",
    "feature_importance = list(zip(X_train.columns, best_clf.feature_importances_))\n",
    "\n",
    "# 모델을 저장합니다.\n",
    "joblib.dump(best_clf, \"separation.pkl\")\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(f\"{rnd_search.best_params_}\\n\")\n",
    "print(\"<10-fold cross-validation>\")\n",
    "print(\"accuracy score mean: \", cv_mean_score)\n",
    "print(\"\\n<AI model: machine learning done >\")\n",
    "print(\"accuracy_score of train data(0.8 of sample): \", train_score)\n",
    "print(\"accuracy_score of test data(0.2 of sample): \", test_score)\n",
    "print(\"\\n<Confusion matrix>\")\n",
    "print(\"(of test)\")\n",
    "print(\"up\", \"neutral\", \"down\")\n",
    "print(cm_test)\n",
    "print(\"(of all)\")\n",
    "print(\"up\", \"neutral\", \"down\")\n",
    "print(cm_all)\n",
    "print(\"\\n<Feature importance>\")\n",
    "for name, score in feature_importance:\n",
    "    print(name, \": \", score)\n",
    "print(\"\\n< AI model: save >\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\성현태\\AppData\\Local\\Temp\\ipykernel_21780\\3881471841.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  updated_updown['Date'] = pd.to_datetime(updated_updown['Date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1333 entries, 0 to 1332\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Date           1333 non-null   datetime64[ns]\n",
      " 1   WTI            1333 non-null   float64       \n",
      " 2   USDKRW         1333 non-null   float64       \n",
      " 3   VIX            1333 non-null   float64       \n",
      " 4   1              1333 non-null   float64       \n",
      " 5   2              1333 non-null   float64       \n",
      " 6   3              1333 non-null   float64       \n",
      " 7   4              1333 non-null   float64       \n",
      " 8   5              1333 non-null   float64       \n",
      " 9   6              1333 non-null   float64       \n",
      " 10  7              1333 non-null   float64       \n",
      " 11  8              1333 non-null   float64       \n",
      " 12  9              1333 non-null   float64       \n",
      " 13  10             1333 non-null   float64       \n",
      " 14  11             1333 non-null   float64       \n",
      " 15  per            1333 non-null   float64       \n",
      " 16  pbr            1333 non-null   float64       \n",
      " 17  forward_stage  1333 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(16), object(1)\n",
      "memory usage: 187.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        Date   WTI  USDKRW   VIX    1    2    3    4    5    6    7    8    9  \\\n",
       " 0 2011-04-13  1.00    -1.7 -0.65  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       " 1 2011-04-14  1.55     0.0 -0.95  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       " 2 2011-04-18  1.03     0.8 -1.13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       " 3 2011-04-19  3.30    -5.8 -0.76  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       " 4 2011-05-16 -0.46     4.6 -0.69  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       " \n",
       "     10   11   per   pbr forward_stage  \n",
       " 0  0.0  0.0  0.17  0.01          down  \n",
       " 1  0.0  0.0 -0.02  0.00            up  \n",
       " 2  0.0  0.0 -0.13 -0.01          down  \n",
       " 3  0.0  0.0  0.44  0.04          down  \n",
       " 4  0.0  0.0 -0.01  0.00            up  ,\n",
       " None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "merged_data = pd.read_csv('merged_data.csv')\n",
    "updated_updown = pd.read_csv('updated_updown.csv')\n",
    "\n",
    "# Ensure the 'Date' column is of datetime type for both DataFrames\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "updated_updown['Date'] = pd.to_datetime(updated_updown['Date'])\n",
    "\n",
    "# Merging the data\n",
    "# Keep all columns from merged_data, replace 'forward_stage' column with the one from updated_updown\n",
    "final_data = pd.merge(merged_data.drop(columns=['forward_stage']), \n",
    "                      updated_updown[['Date', 'forward_stage']], \n",
    "                      on='Date', how='left')\n",
    "\n",
    "# Checking the first few rows of the final data\n",
    "final_data.head(), final_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def 열_삭제_후_저장(input_file_path, output_file_path, column_to_drop):\n",
    "    \"\"\"\n",
    "    이 함수는 input_file_path로부터 CSV 파일을 읽어와서 지정된 열을 삭제하고,\n",
    "    output_file_path로 새로운 CSV 파일을 저장합니다.\n",
    "    \n",
    "    :param input_file_path: str, 입력 CSV 파일의 경로\n",
    "    :param output_file_path: str, 새 CSV 파일을 저장할 경로\n",
    "    :param column_to_drop: str, 삭제할 열의 이름\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 데이터를 불러옵니다.\n",
    "        df = pd.read_csv(input_file_path, encoding='cp949')\n",
    "        \n",
    "        # 지정된 열을 삭제합니다.\n",
    "        df.drop(columns=[column_to_drop], inplace=True)\n",
    "        \n",
    "        # 변경된 데이터를 새로운 CSV 파일로 저장합니다.\n",
    "        df.to_csv(output_file_path, index=False, encoding='cp949')\n",
    "        \n",
    "        print(f\"{column_to_drop} 열이 삭제되었고, 새 CSV는 {output_file_path}에 저장되었습니다.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"에러가 발생했습니다: {e}\")\n",
    "\n",
    "# 함수 사용 예시:\n",
    "input_file_path = 'feature.csv'\n",
    "output_file_path = 'feature_new.csv'\n",
    "column_to_drop = '삭제할_열_이름'  # 여기에 삭제하려는 열의 이름을 입력합니다.\n",
    "\n",
    "열_삭제_후_저장(input_file_path, output_file_path, column_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 57, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 367, in fit\n",
      "    y, expanded_class_weight = self._validate_y_class_weight(y)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 783, in _validate_y_class_weight\n",
      "    expanded_class_weight = compute_sample_weight(class_weight, y_original)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 167, in compute_sample_weight\n",
      "    weight_k = compute_class_weight(\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 62, in compute_class_weight\n",
      "    i = np.searchsorted(classes, c)\n",
      "  File \"<__array_function__ internals>\", line 5, in searchsorted\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 1350, in searchsorted\n",
      "    return _wrapfunc(a, 'searchsorted', v, side=side, sorter=sorter)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 66, in _wrapfunc\n",
      "    return _wrapit(obj, method, *args, **kwds)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 43, in _wrapit\n",
      "    result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.36831683 0.37920792 0.40924092 0.40231023 0.40627063\n",
      " 0.37062706        nan 0.37326733 0.40891089]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_leaf_nodes': 60, 'max_features': 1, 'max_depth': 20, 'class_weight': None}\n",
      "\n",
      "<10-fold cross-validation>\n",
      "accuracy score mean:  0.4092409240924092\n",
      "\n",
      "<AI model: machine learning done >\n",
      "accuracy_score of train data(0.8 of sample):  0.8471681150734193\n",
      "accuracy_score of test data(0.2 of sample):  0.6546762589928058\n",
      "\n",
      "<Confusion matrix>\n",
      "(of test)\n",
      "up neutral down\n",
      "[[176   0 122]\n",
      " [ 20   0 134]\n",
      " [ 12   0 370]]\n",
      "(of all)\n",
      "up neutral down\n",
      "[[1230   86  145]\n",
      " [ 175  488  283]\n",
      " [  54   55 1655]]\n",
      "\n",
      "<Feature importance>\n",
      "WTI :  0.1116420697840777\n",
      "USDKRW :  0.11494926996927929\n",
      "VIX :  0.10166662084407262\n",
      "M2 :  0.1290566254636067\n",
      "1 :  0.045615676228569095\n",
      "2 :  0.11209548512539244\n",
      "3 :  0.09119740362460131\n",
      "4 :  0.10531184321566138\n",
      "5 :  0.08868556829703704\n",
      "6 :  0.04396373247667015\n",
      "7 :  0.019748764773851294\n",
      "8 :  0.03606694019718076\n",
      "\n",
      "< AI model: save >\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로딩\n",
    "model_data = pd.read_csv('feature.csv', encoding='cp949')\n",
    "\n",
    "# 'Date' 칼럼을 인덱스로 설정하고 datetime 타입으로 변환\n",
    "model_data['Date'] = pd.to_datetime(model_data['Date'])\n",
    "model_data.set_index('Date', inplace=True)\n",
    "\n",
    "# 데이터를 날짜 기준으로 정렬\n",
    "model_data.sort_index(inplace=True)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 나눌 기준 날짜를 설정\n",
    "split_date = model_data.index[int(0.8 * len(model_data))]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 나눔\n",
    "train_df = model_data[model_data.index <= split_date]\n",
    "test_df = model_data[model_data.index > split_date]\n",
    "\n",
    "# 특성과 레이블을 준비\n",
    "X_train = train_df.drop(columns=['forward_stage'])\n",
    "y_train = train_df['forward_stage']\n",
    "X_test = test_df.drop(columns=['forward_stage'])\n",
    "y_test = test_df['forward_stage']\n",
    "\n",
    "# 클래스 불균형 문제를 해결하기 위해 클래스 가중치를 계산\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# 모델 초기화\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=42, class_weight=class_weight_dict)\n",
    "\n",
    "# 하이퍼파라미터 그리드를 확장\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_leaf_nodes': [20, 30, 40, 50, 60],\n",
    "    'max_features': [1, 2, 3, 4],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': [class_weight_dict, None]\n",
    "}\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위해 RandomizedSearchCV를 사용\n",
    "time_split_cv = TimeSeriesSplit(n_splits=10)\n",
    "rnd_search = RandomizedSearchCV(rnd_clf, param_dist_rf, cv=time_split_cv, random_state=42)\n",
    "rnd_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델을 훈련\n",
    "best_clf = rnd_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# 모델 평가\n",
    "train_score = best_clf.score(X_train, y_train)\n",
    "test_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "# 10-fold cross-validation의 정확도를 계산\n",
    "cv_scores = cross_val_score(best_clf, X_train, y_train, cv=time_split_cv, scoring='accuracy')\n",
    "cv_mean_score = cv_scores.mean()\n",
    "\n",
    "# 결과 출력 및 모델 저장\n",
    "\n",
    "# 혼동 행렬을 확인합니다.\n",
    "y_test_pred = best_clf.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "y_all_pred = best_clf.predict(model_data.drop(columns=['forward_stage']))\n",
    "cm_all = confusion_matrix(model_data['forward_stage'], y_all_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "\n",
    "# 특성 중요도를 확인합니다.\n",
    "feature_importance = list(zip(X_train.columns, best_clf.feature_importances_))\n",
    "\n",
    "# 모델을 저장합니다.\n",
    "joblib.dump(best_clf, \"separation11.pkl\")\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(f\"{rnd_search.best_params_}\\n\")\n",
    "print(\"<10-fold cross-validation>\")\n",
    "print(\"accuracy score mean: \", cv_mean_score)\n",
    "print(\"\\n<AI model: machine learning done >\")\n",
    "print(\"accuracy_score of train data(0.8 of sample): \", train_score)\n",
    "print(\"accuracy_score of test data(0.2 of sample): \", test_score)\n",
    "print(\"\\n<Confusion matrix>\")\n",
    "print(\"(of test)\")\n",
    "print(\"up\", \"neutral\", \"down\")\n",
    "print(cm_test)\n",
    "print(\"(of all)\")\n",
    "print(\"up\", \"neutral\", \"down\")\n",
    "print(cm_all)\n",
    "print(\"\\n<Feature importance>\")\n",
    "for name, score in feature_importance:\n",
    "    print(name, \": \", score)\n",
    "print(\"\\n< AI model: save >\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7235901509134234,\n",
       " 0.5558375634517766,\n",
       " array([[  0,   0,  85],\n",
       "        [  0,   0,  90],\n",
       "        [  0,   0, 219]], dtype=int64),\n",
       " array([[1168,   12,  281],\n",
       "        [ 260,  131,  555],\n",
       "        [  87,   24, 1653]], dtype=int64),\n",
       " [('WTI', 0.10854812866140856),\n",
       "  ('USDKRW', 0.1426964236443898),\n",
       "  ('VIX', 0.16929253948500653),\n",
       "  ('M2', 0.11800574910278497),\n",
       "  ('1', 0.0263733766352262),\n",
       "  ('2', 0.11231968616666016),\n",
       "  ('3', 0.08294082561505137),\n",
       "  ('4', 0.1140233703912902),\n",
       "  ('5', 0.07254457752539362),\n",
       "  ('6', 0.011181101686514833),\n",
       "  ('7', 0.008959586691127954),\n",
       "  ('8', 0.0331146343951458)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn.metrics as mt \n",
    "from sklearn.tree import export_graphviz \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "import joblib \n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# Loading the data again after ensuring the correct file path is used\n",
    "model_data = pd.read_csv('feature.csv', encoding='cp949')\n",
    "\n",
    "# Setting 'Date' column as index and converting it to datetime\n",
    "model_data['Date'] = pd.to_datetime(model_data['Date'])\n",
    "model_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Sorting data by date\n",
    "model_data.sort_index(inplace=True)\n",
    "\n",
    "# Define a split date. As we don't have a user-provided split date, using a placeholder.\n",
    "# The user should replace it with the actual split date.\n",
    "split_date = '2022-01-03'  \n",
    "\n",
    "# Splitting the data into train and test\n",
    "train_df = model_data[model_data.index <= split_date]\n",
    "test_df = model_data[model_data.index > split_date]\n",
    "\n",
    "# Preparing features and labels\n",
    "X_train = train_df.drop(columns=['forward_stage'])\n",
    "y_train = train_df['forward_stage']\n",
    "X_test = test_df.drop(columns=['forward_stage'])\n",
    "y_test = test_df['forward_stage']\n",
    "\n",
    "# Initializing the model\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 500],\n",
    "    'max_leaf_nodes': [20, 30, 40, 50],\n",
    "    'max_features': [1, 2, 3],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning\n",
    "rnd_search = RandomizedSearchCV(rnd_clf, param_dist_rf, cv=10, random_state=42)\n",
    "rnd_search.fit(X_train, y_train)\n",
    "\n",
    "# Train the model with best parameters\n",
    "best_clf = rnd_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = best_clf.score(X_train, y_train)\n",
    "test_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "# Confusion matrix\n",
    "y_test_pred = best_clf.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "y_all_pred = best_clf.predict(model_data.drop(columns=['forward_stage']))\n",
    "cm_all = confusion_matrix(model_data['forward_stage'], y_all_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = list(zip(X_train.columns, best_clf.feature_importances_))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_clf, \"forecast_model.pkl\")\n",
    "\n",
    "train_score, test_score, cm_test, cm_all, feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.704225352112676,\n",
       " 0.6342925659472423,\n",
       " array([[163,   0, 135],\n",
       "        [ 21,   0, 133],\n",
       "        [ 16,   0, 366]], dtype=int64),\n",
       " array([[1053,   36,  372],\n",
       "        [ 229,  198,  519],\n",
       "        [ 113,   23, 1628]], dtype=int64),\n",
       " [('WTI', 0.10769227613713275),\n",
       "  ('USDKRW', 0.1054543026001435),\n",
       "  ('VIX', 0.10868732316066305),\n",
       "  ('M2', 0.09405144083259712),\n",
       "  ('1', 0.04860205681700061),\n",
       "  ('2', 0.12072185830751007),\n",
       "  ('3', 0.1091050480849538),\n",
       "  ('4', 0.12483697609451415),\n",
       "  ('5', 0.06459268812687957),\n",
       "  ('6', 0.04384885691719701),\n",
       "  ('7', 0.02692945277683438),\n",
       "  ('8', 0.04547772014457407)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# 올바른 파일 경로로 데이터를 불러옵니다.\n",
    "model_data = pd.read_csv('feature.csv', encoding='cp949')\n",
    "\n",
    "# 'Date' 칼럼을 인덱스로 설정하고 datetime 타입으로 변환합니다.\n",
    "model_data['Date'] = pd.to_datetime(model_data['Date'])\n",
    "model_data.set_index('Date', inplace=True)\n",
    "\n",
    "# 데이터를 날짜 기준으로 정렬합니다.\n",
    "model_data.sort_index(inplace=True)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 나눌 기준 날짜를 설정합니다.\n",
    "# 사용자가 지정한 split_date가 없기 때문에 데이터셋의 80%에 해당하는 날짜를 자동으로 선택합니다.\n",
    "split_date = model_data.index[int(0.8 * len(model_data))]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 나눕니다.\n",
    "train_df = model_data[model_data.index <= split_date]\n",
    "test_df = model_data[model_data.index > split_date]\n",
    "\n",
    "# 특성과 레이블을 준비합니다.\n",
    "X_train = train_df.drop(columns=['forward_stage'])\n",
    "y_train = train_df['forward_stage']\n",
    "X_test = test_df.drop(columns=['forward_stage'])\n",
    "y_test = test_df['forward_stage']\n",
    "\n",
    "# 모델을 초기화합니다.\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=42)\n",
    "\n",
    "# 하이퍼파라미터 그리드를 정의합니다.\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 500],\n",
    "    'max_leaf_nodes': [20, 30, 40, 50],\n",
    "    'max_features': [1, 2, 3],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위해 RandomizedSearchCV를 사용합니다.\n",
    "rnd_search = RandomizedSearchCV(rnd_clf, param_dist_rf, cv=10, random_state=42)\n",
    "rnd_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델을 훈련합니다.\n",
    "best_clf = rnd_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# 모델을 평가합니다.\n",
    "train_score = best_clf.score(X_train, y_train)\n",
    "test_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "# 혼동 행렬을 확인합니다.\n",
    "y_test_pred = best_clf.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "y_all_pred = best_clf.predict(model_data.drop(columns=['forward_stage']))\n",
    "cm_all = confusion_matrix(model_data['forward_stage'], y_all_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "\n",
    "# 특성 중요도를 확인합니다.\n",
    "feature_importance = list(zip(X_train.columns, best_clf.feature_importances_))\n",
    "\n",
    "# 모델을 저장합니다.\n",
    "joblib.dump(best_clf, \"separation.pkl\")\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "train_score, test_score, cm_test, cm_all, feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data again with the correct file path this time\n",
    "model_data = pd.read_csv('feature.csv', encoding='cp949')\n",
    "\n",
    "# Setting 'Date' column as index and converting it to datetime\n",
    "model_data['Date'] = pd.to_datetime(model_data['Date'])\n",
    "model_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Sorting data by date\n",
    "model_data.sort_index(inplace=True)\n",
    "\n",
    "# Define a split date. As we don't have a user-provided split date, using a placeholder.\n",
    "# The user should replace it with the actual split date.\n",
    "split_date = '2022-01-03'  \n",
    "\n",
    "# Splitting the data into train and test based on the split date\n",
    "train_df = model_data[model_data.index <= split_date]\n",
    "test_df = model_data[model_data.index > split_date]\n",
    "\n",
    "# Preparing features and labels\n",
    "X_train = train_df.drop(columns=['forward_stage'])\n",
    "y_train = train_df['forward_stage']\n",
    "X_test = test_df.drop(columns=['forward_stage'])\n",
    "y_test = test_df['forward_stage']\n",
    "\n",
    "# Initializing the model\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 500],\n",
    "    'max_leaf_nodes': [20, 30, 40, 50],\n",
    "    'max_features': [1, 2, 3],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning\n",
    "rnd_search = RandomizedSearchCV(rnd_clf, param_dist_rf, cv=10, random_state=42)\n",
    "rnd_search.fit(X_train, y_train)\n",
    "\n",
    "# Train the model with best parameters\n",
    "best_clf = rnd_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = best_clf.score(X_train, y_train)\n",
    "test_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "# Confusion matrix\n",
    "y_test_pred = best_clf.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "y_all_pred = best_clf.predict(model_data.drop(columns=['forward_stage']))\n",
    "cm_all = confusion_matrix(model_data['forward_stage'], y_all_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = list(zip(X_train.columns, best_clf.feature_importances_))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_clf, \"1.pkl\")\n",
    "\n",
    "train_score, test_score, cm_test, cm_all, feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import sklearn.metrics as mt\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit, RandomizedSearchCV, cross_val_score\n",
    "# import joblib\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # 1. 수정된 CSV 데이터 로드\n",
    "# model_data = pd.read_csv(\"3.csv\")\n",
    "\n",
    "# # 'Date' 컬럼을 인덱스로 설정\n",
    "# model_data.set_index('Date', inplace=True)\n",
    "\n",
    "# # 2. 피처와 라벨의 완전한 데이터 생성\n",
    "# # 'forward_stage' 컬럼을 제외한 나머지를 피처로 사용\n",
    "# X = model_data.drop(columns=['forward_stage'])\n",
    "# # 'forward_stage' 컬럼을 라벨로 사용\n",
    "# y = model_data['forward_stage']\n",
    "\n",
    "# # 과거 데이터만 선택\n",
    "# X_past = X[y.notna()]\n",
    "# y_past = y[y.notna()]\n",
    "\n",
    "# # 3. 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# for train_index, test_index in sss.split(X_past, y_past):\n",
    "#     X_train, X_test = X_past.iloc[train_index,], X_past.iloc[test_index,]\n",
    "#     y_train, y_test = y_past.iloc[train_index], y_past.iloc[test_index]\n",
    "\n",
    "# # 4. 모델 파인튜닝: 최적의 하이퍼파라미터 찾기\n",
    "# rnd_clf = RandomForestClassifier(n_jobs=1, random_state=42)  # n_jobs를 1로 설정하여 멀티프로세싱을 사용하지 않음\n",
    "\n",
    "# # 탐색할 하이퍼파라미터 설정\n",
    "# param_dist_rf = {\n",
    "#     'n_estimators': [50, 100, 500],  # 트리의 개수\n",
    "#     'max_leaf_nodes': [20, 30, 40, 50],  # 리프 노드의 최대 개수\n",
    "#     'max_features': [1, 2, 3],  # 최대 피처 개수\n",
    "#     'max_depth': [5, 10, 15],  # 최대 깊이\n",
    "#     'min_samples_split': [2, 5, 10],  # 노드를 분할하기 위한 최소 샘플 개수\n",
    "#     'min_samples_leaf': [1, 2, 4]  # 리프 노드에 있어야 하는 최소 샘플 개수\n",
    "# }\n",
    "\n",
    "# # RandomizedSearchCV를 사용하여 최적의 하이퍼파라미터 찾기\n",
    "# rnd_search = RandomizedSearchCV(rnd_clf, param_dist_rf, cv=10, random_state=42)\n",
    "# rnd_search.fit(X_train, y_train)\n",
    "# print(\"최적의 파라미터: \", rnd_search.best_params_)\n",
    "\n",
    "# # 5. 모델 훈련 및 K-겹 교차 검증을 사용한 평가\n",
    "# rnd_scores = cross_val_score(rnd_clf, X_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "# print(\"\\n<10-겹 교차 검증>\")\n",
    "# print(\"정확도 평균: \", rnd_scores.mean())\n",
    "\n",
    "# # 6. 최종 모델 훈련\n",
    "# rnd_clf.fit(X_train, y_train)\n",
    "# print(\"\\n<AI 모델: 학습 완료>\")\n",
    "# print(\"훈련 데이터 정확도(샘플의 0.8): \", rnd_clf.score(X_train, y_train))\n",
    "\n",
    "# # 7. 테스트 데이터에서 모델 평가\n",
    "# print(\"테스트 데이터 정확도(샘플의 0.2): \", rnd_clf.score(X_test, y_test))\n",
    "\n",
    "# # 8. 혼동 행렬 확인\n",
    "# y_test_pred = rnd_clf.predict(X_test)\n",
    "# cm1 = confusion_matrix(y_test, y_test_pred, labels=[\"up\", \"neutral\", \"down\"])\n",
    "# print(\"\\n<혼동 행렬>\")\n",
    "# print(\"(테스트 데이터)\")\n",
    "# print(\"상승\", \"보합\", \"하락\")\n",
    "# print(cm1)\n",
    "# cm2 = confusion_matrix(y_past, rnd_clf.predict(X_past), labels=[\"up\", \"neutral\", \"down\"])\n",
    "# print(\"(전체 데이터)\")\n",
    "# print(\"상승\", \"보합\", \"하락\")\n",
    "# print(cm2)\n",
    "\n",
    "# # 9. 피처 중요도 확인\n",
    "# print(\"\\n<피처 중요도>\")\n",
    "# for name, score in zip(X.columns, rnd_clf.feature_importances_):\n",
    "#     print(name, \": \", score)\n",
    "\n",
    "# # 10. 백테스팅을 위한 예측 데이터 생성\n",
    "# y_prediction = rnd_clf.predict(X)\n",
    "# y_pred = pd.Series(y_prediction, index=y.index)\n",
    "\n",
    "# # 11. 모델 저장\n",
    "# joblib.dump(rnd_clf, \"forecast_model.pkl\")\n",
    "# print(\"\\n< AI 모델: 저장 완료>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "07-01-02    neutral\n",
       "07-01-03    neutral\n",
       "07-01-04         up\n",
       "07-01-05         up\n",
       "07-01-08         up\n",
       "             ...   \n",
       "16-11-24         up\n",
       "16-12-26         up\n",
       "17-01-02         up\n",
       "17-01-16         up\n",
       "17-02-20         up\n",
       "Length: 4303, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml2.py code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. 모델 로드\n",
    "rnd_clf = joblib.load(\"forecast_model.pkl\")\n",
    "print(\"\\n< AI model: load >\")\n",
    "\n",
    "# 2. new daily raw data 가져오기\n",
    "model_data = pd.read_csv(\"3.csv\")\n",
    "\n",
    "# 'forward_stage' 컬럼을 숫자로 변환합니다.\n",
    "categories = model_data['forward_stage'].astype('category')\n",
    "model_data['forward_stage'] = categories.cat.codes\n",
    "\n",
    "# X는 'forward_stage'와 'Date'를 제외한 모든 컬럼을 포함해야 합니다.\n",
    "X = model_data.drop(columns=['forward_stage', 'Date'])\n",
    "X.columns = X.columns.astype(str)  # Set feature names to avoid warning\n",
    "y = model_data[\"forward_stage\"]\n",
    "\n",
    "# y가 NaN이 아닌 행만 선택합니다.\n",
    "X_past = X[y.notna()]\n",
    "y_past = y[y.notna()]\n",
    "\n",
    "# 3. new daily raw data 전체 학습\n",
    "rnd_clf.fit(X_past, y_past)\n",
    "print(\"\\n< AI model: machine learning done >\")\n",
    "print(\"accuracy_score of whole data: \", rnd_clf.score(X_past, y_past))\n",
    "\n",
    "# 4. 현재(마지막) 데이터 표시\n",
    "print(\"\\n<Current status>\")\n",
    "for col, score in zip(X.columns, X.iloc[-1]):\n",
    "    print(\"{:20} : {:>8.3f}\".format(col, score))\n",
    "\n",
    "X_current = np.array(X.iloc[-1]).reshape(1, -1)\n",
    "\n",
    "# 5. 현재 전망\n",
    "print(\"\\n< AI model: forecasting >\")\n",
    "y_current_pred = rnd_clf.predict(X_current)\n",
    "print(\"forecast: \", categories.cat.categories[y_current_pred[0]])\n",
    "\n",
    "# 현재전망의 확률표\n",
    "prob_current = rnd_clf.predict_proba(X_current)\n",
    "y_names = rnd_clf.classes_\n",
    "print(\"\\n[class] : [prob]\")\n",
    "for name, prob in zip(categories.cat.categories[y_names], prob_current[0]):\n",
    "    print(\"{:7} : {:.2f}\".format(name, prob))\n",
    "\n",
    "# # 6. 2023년 일별 전망치의 확률 변화\n",
    "# # 전기간 전망치 확률 데이터생성\n",
    "# prob = rnd_clf.predict_proba(X)\n",
    "# prob_df = pd.DataFrame(prob, columns=categories.cat.categories)\n",
    "\n",
    "# # '2023'이라는 컬럼이나 인덱스는 없으므로, 'Date' 컬럼에서 '2023'년에 해당하는 데이터를 선택해야 합니다.\n",
    "# model_data['Year'] = pd.to_datetime(model_data['Date'], format='%y-%m-%d').dt.year\n",
    "# prob_2023 = prob_df[model_data['Year'] == 2023]\n",
    "\n",
    "# # '2023'년에 해당하는 날짜를 가져옵니다.\n",
    "# dates_2023 = model_data.loc[model_data['Year'] == 2023, 'Date']\n",
    "\n",
    "# plt.bar(dates_2023, prob_2023['up'], label='up', color='r')\n",
    "# plt.bar(dates_2023, prob_2023['neutral'], label='neutral', color='g', bottom=prob_2023['up'])\n",
    "# plt.bar(dates_2023, prob_2023['down'], label='down', color='b', bottom=prob_2023[['up', 'neutral']].sum(axis=1))\n",
    "# plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "# -1. MDD 함수 정의\n",
    "def MDD(list_values): \n",
    " mdd_value = 0 \n",
    " for i in range(1, len(list_values)): \n",
    "    bw_max = max(list_values[:i]) \n",
    "    curr = list_values[i] \n",
    "    mdd = curr / bw_max - 1 \n",
    " if mdd < mdd_value: \n",
    "    mdd_value = mdd \n",
    " return mdd_value \n",
    "# 0. 사후방향성 클래스를 수치로 전환하는 함수 정의 (up=1, neutral=0, down=-1) \n",
    "def convert_num(pred): \n",
    "    pred_num = np.empty(len(pred)) \n",
    "    pred_num[pred=='up']=1 \n",
    "    pred_num[pred=='neutral']=0 \n",
    "    pred_num[pred=='down']=-1 \n",
    "    pred_num[pred.isna()]=np.NaN \n",
    "    \n",
    "    return pred_num \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'2007-04-29'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3653\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3654\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '2007-04-29'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8364\\1014580719.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mkdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stage'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;31m#말일자 아님\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m  \u001b[0mkdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'stage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'before_last'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# 3. 전월말 투자의견 열 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mkdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pre_stage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mkdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1116\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3654\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3655\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3656\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3657\u001b[0m             \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '2007-04-29'"
     ]
    }
   ],
   "source": [
    "# backtest.py code\n",
    "# 1. 월간 시장수익률 데이터 가져오기\n",
    "# 데이터는 일자/월수익률(원수치)/코스피지수/월말하루전영업일로 구성\n",
    "kdata = pd.read_csv(\"investing.csv\")\n",
    "# 2. AI모델로 예측한 예측정보를 가져오기\n",
    "# 기존 AI모델 학습 프로세스에서 만들어낸 y_pred 데이터를 조회함. \n",
    "# 매월말에 예측했던 투자의견을 가져옴. 단, 실제 투자를 위해서, 월말하루전영업일 기준 자료를 가져옴\n",
    "# kdata['stage']='' \n",
    "kdata['stage'] = y_pred #말일자 아님\n",
    "for index in kdata.index: \n",
    " kdata.loc[index,'stage']=y_pred[kdata.loc[index,'before_last']] \n",
    "# 3. 전월말 투자의견 열 생성\n",
    "kdata['pre_stage']= kdata['stage'].shift(1) \n",
    "kdata['port_return']=0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. 전략 수익률 생성\n",
    "# 전월말 투자의견이 상승이면 코스피 long, 보합이면 Cash, 하락이면 코스피 Short 실행. \n",
    "# 해당 전략에 따라 포트 월별수익률(port_return) 생성\n",
    "kdata.loc[kdata['pre_stage']=='up', 'port_return'] = kdata['m_return']*1 \n",
    "kdata.loc[kdata['pre_stage'].isna(), 'port_return'] = kdata['m_return']*1 \n",
    "kdata.loc[kdata['pre_stage']=='neutral', 'port_return'] = 0 \n",
    "kdata.loc[kdata['pre_stage']=='down', 'port_return'] = kdata['m_return']*-1 \n",
    "# 코스피와 모델포트폴리오의 누적수익률(1에서 시작하는 인덱스 형태) 생성\n",
    "kdata['kospi_cumul']=(1+kdata['m_return']).cumprod() \n",
    "kdata['port_cumul']=(1+kdata['port_return']).cumprod() \n",
    "# 5. 백테스팅 결과 기록(CAGR, 변동성, Sharpe ratio, MDD) \n",
    "my_back = {'months':len(kdata)} \n",
    "\n",
    "my_back['k_cumul_return_idx']=kdata['kospi_cumul'][-1] \n",
    "my_back['k_cumul_return_pct']=(my_back['k_cumul_return_idx']-1)*100 \n",
    "my_back['k_cagr']=(my_back['k_cumul_return_idx']**(12/my_back['months']))-1 \n",
    "my_back['k_cagr_pct']=my_back['k_cagr']*100 \n",
    "my_back['k_vol_pct']=np.std(kdata['m_return'])*np.sqrt(12)*100 \n",
    "my_back['k_Sharpe']=my_back['k_cagr_pct']/my_back['k_vol_pct'] \n",
    "my_back['k_MDD']=MDD(kdata['kospi_cumul'])*100 \n",
    "\n",
    "my_back['port_cumul_return_idx']=kdata['port_cumul'][-1] \n",
    "my_back['port_cumul_return_pct']=(my_back['port_cumul_return_idx']-1)*100 \n",
    "my_back['port_cagr']=(my_back['port_cumul_return_idx']**(12/my_back['months']))-1 \n",
    "my_back['port_cagr_pct']=my_back['port_cagr']*100 \n",
    "my_back['port_vol_pct']=np.std(kdata['port_return'])*np.sqrt(12)*100 \n",
    "my_back['port__Sharpe']=my_back['port_cagr_pct']/my_back['port_vol_pct'] \n",
    "my_back['port_MDD']=MDD(kdata['port_cumul'])*100 \n",
    "\n",
    "# 6. 백테스팅 결과 출력하기\n",
    "print(\"<Backtesting result>\") \n",
    "for key, value in my_back.items(): \n",
    " print(\"{:22}: {:>8.3f}\".format(key, value)) \n",
    " \n",
    "# 포트폴리오 누적수익률 그래프\n",
    "kdata['port_cumul'].plot() \n",
    "plt.title('Portfolio performance index') \n",
    "plt.ylabel('\\'02/12/31 = 1') \n",
    "plt.show() \n",
    "\n",
    "# 7. 월말 모델전망치와 실제결과치 출력\n",
    "# 월말 실제결과치 입력\n",
    "for index in kdata.index: \n",
    "\n",
    " kdata.loc[index,'real_stage']=y[kdata.loc[index,'before_last']] \n",
    "# 사후방향성 클래스를 수치로 변환\n",
    "kdata['stage_num']=convert_num(kdata['stage']) \n",
    "kdata['real_stage_num']=convert_num(kdata['real_stage']) \n",
    "\n",
    "# 전망치와 결과치의 그래프 출력\n",
    "kdata.plot(y=['stage_num', 'real_stage_num'], label=['model forecast','real direction']) \n",
    "plt.title('Model forecast vs. Real direction') \n",
    "plt.ylabel('up=1, neutral=0, down=-1') \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
